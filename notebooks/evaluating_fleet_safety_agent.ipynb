{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fleet Safety Agent Evaluation\n",
        "\n",
        "This notebook demonstrates how to evaluate the Fleet Safety Agent using:\n",
        "1. **Local Testing** - Direct agent invocation\n",
        "2. **Vertex AI Gen AI Evaluation** - Trajectory and response metrics\n",
        "\n",
        "> **Note**: Use the same `.venv` created by `make install` to ensure dependency compatibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "sys.path.insert(0, os.path.abspath(\"..\"))\n",
        "\n",
        "# Load environment variables using the project's env helper\n",
        "from app.helpers.env import load_env_and_verify_api_key\n",
        "\n",
        "# This loads .env from the project root and verifies GOOGLE_API_KEY is set\n",
        "# Set require_maps_key=True if testing route planning features\n",
        "_ = load_env_and_verify_api_key(require_maps_key=True)\n",
        "print(\"Environment loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# ADK imports\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.genai import types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the Fleet Safety Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent loaded: fleet_safety_orchestrator\n",
            "App name: fleet_safety_agent\n"
          ]
        }
      ],
      "source": [
        "# Import the Fleet Safety Orchestrator\n",
        "from app.agent import orchestrator, app\n",
        "\n",
        "print(f\"Agent loaded: {orchestrator.name}\")\n",
        "print(f\"App name: {app.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_events_to_dict(events: list, *, as_json: bool = False) -> dict:\n",
        "    \"\"\"\n",
        "    Parse ADK events into a structured dictionary with response and trajectory.\n",
        "    \"\"\"\n",
        "    final_response = \"\"\n",
        "    trajectory = []\n",
        "\n",
        "    for event in events:\n",
        "        if not getattr(event, \"content\", None) or not getattr(event.content, \"parts\", None):\n",
        "            continue\n",
        "        for part in event.content.parts:\n",
        "            if getattr(part, \"function_call\", None):\n",
        "                info = {\n",
        "                    \"tool_name\": part.function_call.name,\n",
        "                    \"tool_input\": dict(part.function_call.args),\n",
        "                }\n",
        "                if info not in trajectory:\n",
        "                    trajectory.append(info)\n",
        "            if event.content.role == \"model\" and getattr(part, \"text\", None):\n",
        "                final_response = part.text.strip()\n",
        "\n",
        "    return {\n",
        "        \"response\": final_response,\n",
        "        \"predicted_trajectory\": json.dumps(trajectory) if as_json else trajectory\n",
        "    }\n",
        "\n",
        "\n",
        "def format_as_markdown(output: dict) -> str:\n",
        "    \"\"\"Convert output to formatted markdown.\"\"\"\n",
        "    md = f\"### Response\\n{output['response']}\\n\\n\"\n",
        "    if output[\"predicted_trajectory\"]:\n",
        "        md += \"### Tool Calls\\n\"\n",
        "        traj = output[\"predicted_trajectory\"]\n",
        "        if isinstance(traj, str):\n",
        "            traj = json.loads(traj)\n",
        "        for call in traj:\n",
        "            md += f\"- **{call['tool_name']}**\\n\"\n",
        "            for k, v in call.get(\"tool_input\", {}).items():\n",
        "                md += f\"  - `{k}`: `{v}`\\n\"\n",
        "    return md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Agent Runner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_fleet_safety_agent(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Run the Fleet Safety Agent with a query and return parsed results.\n",
        "    \"\"\"\n",
        "    app_name = \"fleet_safety_eval\"\n",
        "    user_id = \"eval_user\"\n",
        "    session_id = f\"eval_session_{hash(query) % 10000}\"\n",
        "\n",
        "    # Create session service\n",
        "    session_service = InMemorySessionService()\n",
        "    await session_service.create_session(\n",
        "        app_name=app_name, user_id=user_id, session_id=session_id\n",
        "    )\n",
        "\n",
        "    # Create runner\n",
        "    runner = Runner(\n",
        "        agent=orchestrator,\n",
        "        app_name=app_name,\n",
        "        session_service=session_service\n",
        "    )\n",
        "\n",
        "    # Run agent\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = [\n",
        "        event async for event in runner.run_async(\n",
        "            user_id=user_id,\n",
        "            session_id=session_id,\n",
        "            new_message=content\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return parse_events_to_dict(events)\n",
        "\n",
        "\n",
        "def run_fleet_safety_agent_sync(prompt: str) -> dict:\n",
        "    \"\"\"Sync wrapper for Vertex AI evaluation.\"\"\"\n",
        "    result = asyncio.run(run_fleet_safety_agent(prompt))\n",
        "    result[\"predicted_trajectory\"] = json.dumps(\n",
        "        result[\"predicted_trajectory\"] if isinstance(result[\"predicted_trajectory\"], list) \n",
        "        else json.loads(result[\"predicted_trajectory\"])\n",
        "    )\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test the Agent Locally\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Query:** Plan a safe route for vehicle v001 from London to Manchester\n",
              "\n",
              "### Response\n",
              "I apologize, but I was unable to plan the route. The `request_route_plan` tool requires a `driver_id` which was not provided. Please provide the driver's ID so I can proceed with planning a safe route for vehicle `v001` from London to Manchester.\n",
              "\n",
              "### Tool Calls\n",
              "- **request_route_plan**\n",
              "  - `destination`: `Manchester`\n",
              "  - `origin`: `London`\n",
              "  - `priority`: `HIGH`\n",
              "  - `vehicle_id`: `v001`\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test Query 1: Route Planning\n",
        "query1 = \"Plan a safe route for vehicle v001 from London to Manchester\"\n",
        "result1 = await run_fleet_safety_agent(query1)\n",
        "display(Markdown(f\"**Query:** {query1}\\n\\n\" + format_as_markdown(result1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Query:** What is the current status of the fleet?\n",
              "\n",
              "### Response\n",
              "The current status of the fleet is as follows:\n",
              "\n",
              "*   **Fleet Size:** 1 vehicle\n",
              "*   **Active Vehicles:** 1\n",
              "*   **Active Trips:** 0\n",
              "*   **Total Alerts:** 0\n",
              "*   **Critical Alerts:** 0\n",
              "*   **System Health:** Good\n",
              "*   **Timestamp:** 2025-11-30T15:24:53.259501\n",
              "\n",
              "### Tool Calls\n",
              "- **get_fleet_status**\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test Query 2: Fleet Status\n",
        "query2 = \"What is the current status of the fleet?\"\n",
        "result2 = await run_fleet_safety_agent(query2)\n",
        "display(Markdown(f\"**Query:** {query2}\\n\\n\" + format_as_markdown(result2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Query:** Check the safety status of vehicle v001\n",
              "\n",
              "### Response\n",
              "The safety status of vehicle `v001` is currently good. The Risk Monitor reports a **low risk level** with a risk score of 0. There are no active alerts and no historical incidents recorded for this vehicle. The safety rating is N/A.\n",
              "\n",
              "### Tool Calls\n",
              "- **check_vehicle_safety**\n",
              "  - `vehicle_id`: `v001`\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test Query 3: Safety Check\n",
        "query3 = \"Check the safety status of vehicle v001\"\n",
        "result3 = await run_fleet_safety_agent(query3)\n",
        "display(Markdown(f\"**Query:** {query3}\\n\\n\" + format_as_markdown(result3)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Vertex AI Gen AI Evaluation (Optional)\n",
        "\n",
        "This section uses Vertex AI's evaluation service for more rigorous testing.\n",
        "\n",
        "> **Prerequisites**: \n",
        "> - GCP Project with Vertex AI enabled\n",
        "> - GCS bucket for evaluation outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip this section if you don't have Vertex AI set up\n",
        "ENABLE_VERTEX_EVAL = False  # Set to True to run Vertex AI evaluation\n",
        "\n",
        "if ENABLE_VERTEX_EVAL:\n",
        "    import vertexai\n",
        "    from google.cloud import aiplatform\n",
        "    from vertexai.preview.evaluation import EvalTask\n",
        "    \n",
        "    # Configure\n",
        "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
        "    LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "    BUCKET_NAME = f\"{PROJECT_ID}-eval-bucket\"\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "    EXPERIMENT_NAME = \"fleet-safety-eval\"\n",
        "    \n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)\n",
        "    print(f\"Vertex AI initialized: {PROJECT_ID} / {LOCATION}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Define Evaluation Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset: 5 test cases\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>reference_trajectory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the current status of the fleet?</td>\n",
              "      <td>[{'tool_name': 'get_fleet_status', 'tool_input...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Plan a safe route from London to Manchester fo...</td>\n",
              "      <td>[{'tool_name': 'request_route_plan', 'tool_inp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Check the safety status of vehicle v001</td>\n",
              "      <td>[{'tool_name': 'check_vehicle_safety', 'tool_i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Generate an executive dashboard for today</td>\n",
              "      <td>[{'tool_name': 'generate_executive_dashboard',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the active alerts in the system?</td>\n",
              "      <td>[{'tool_name': 'get_fleet_status', 'tool_input...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt  \\\n",
              "0           What is the current status of the fleet?   \n",
              "1  Plan a safe route from London to Manchester fo...   \n",
              "2            Check the safety status of vehicle v001   \n",
              "3          Generate an executive dashboard for today   \n",
              "4          What are the active alerts in the system?   \n",
              "\n",
              "                                reference_trajectory  \n",
              "0  [{'tool_name': 'get_fleet_status', 'tool_input...  \n",
              "1  [{'tool_name': 'request_route_plan', 'tool_inp...  \n",
              "2  [{'tool_name': 'check_vehicle_safety', 'tool_i...  \n",
              "3  [{'tool_name': 'generate_executive_dashboard',...  \n",
              "4  [{'tool_name': 'get_fleet_status', 'tool_input...  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fleet Safety specific evaluation dataset\n",
        "eval_data = {\n",
        "    \"prompt\": [\n",
        "        \"What is the current status of the fleet?\",\n",
        "        \"Plan a safe route from London to Manchester for vehicle v001\",\n",
        "        \"Check the safety status of vehicle v001\",\n",
        "        \"Generate an executive dashboard for today\",\n",
        "        \"What are the active alerts in the system?\",\n",
        "    ],\n",
        "    \"reference_trajectory\": [\n",
        "        # Fleet status - should call get_fleet_status\n",
        "        [{\"tool_name\": \"get_fleet_status\", \"tool_input\": {\"include_details\": False}}],\n",
        "        # Route planning - should call request_route_plan\n",
        "        [{\"tool_name\": \"request_route_plan\", \"tool_input\": {\n",
        "            \"origin\": \"London\", \n",
        "            \"destination\": \"Manchester\",\n",
        "            \"vehicle_id\": \"v001\"\n",
        "        }}],\n",
        "        # Safety check - should call check_vehicle_safety\n",
        "        [{\"tool_name\": \"check_vehicle_safety\", \"tool_input\": {\"vehicle_id\": \"v001\"}}],\n",
        "        # Dashboard - should call generate_executive_dashboard\n",
        "        [{\"tool_name\": \"generate_executive_dashboard\", \"tool_input\": {}}],\n",
        "        # Alerts - should call get_fleet_status with details\n",
        "        [{\"tool_name\": \"get_fleet_status\", \"tool_input\": {\"include_details\": True}}],\n",
        "    ],\n",
        "}\n",
        "\n",
        "eval_dataset = pd.DataFrame(eval_data)\n",
        "print(f\"Evaluation dataset: {len(eval_dataset)} test cases\")\n",
        "eval_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Run Trajectory Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertex AI evaluation disabled. Set ENABLE_VERTEX_EVAL = True to run.\n"
          ]
        }
      ],
      "source": [
        "if ENABLE_VERTEX_EVAL:\n",
        "    import random\n",
        "    import string\n",
        "    \n",
        "    def get_id(length=8):\n",
        "        return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "    \n",
        "    # Trajectory metrics\n",
        "    trajectory_metrics = [\n",
        "        \"trajectory_exact_match\",\n",
        "        \"trajectory_in_order_match\",\n",
        "        \"trajectory_any_order_match\",\n",
        "        \"trajectory_precision\",\n",
        "        \"trajectory_recall\",\n",
        "    ]\n",
        "    \n",
        "    EXPERIMENT_RUN = f\"fleet-safety-trajectory-{get_id()}\"\n",
        "    \n",
        "    trajectory_eval_task = EvalTask(\n",
        "        dataset=eval_dataset,\n",
        "        metrics=trajectory_metrics,\n",
        "        experiment=EXPERIMENT_NAME,\n",
        "        output_uri_prefix=BUCKET_URI + \"/trajectory-eval\",\n",
        "    )\n",
        "    \n",
        "    print(\"Running trajectory evaluation...\")\n",
        "    trajectory_result = trajectory_eval_task.evaluate(\n",
        "        runnable=run_fleet_safety_agent_sync,\n",
        "        experiment_run_name=EXPERIMENT_RUN\n",
        "    )\n",
        "    \n",
        "    print(\"\\n### Summary Metrics\")\n",
        "    display(pd.DataFrame(trajectory_result.summary_metrics.items(), columns=[\"Metric\", \"Value\"]))\n",
        "else:\n",
        "    print(\"Vertex AI evaluation disabled. Set ENABLE_VERTEX_EVAL = True to run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Run Response Quality Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertex AI evaluation disabled. Set ENABLE_VERTEX_EVAL = True to run.\n"
          ]
        }
      ],
      "source": [
        "if ENABLE_VERTEX_EVAL:\n",
        "    # Response quality metrics\n",
        "    response_metrics = [\"safety\", \"coherence\"]\n",
        "    \n",
        "    EXPERIMENT_RUN = f\"fleet-safety-response-{get_id()}\"\n",
        "    \n",
        "    response_eval_task = EvalTask(\n",
        "        dataset=eval_dataset,\n",
        "        metrics=response_metrics,\n",
        "        experiment=EXPERIMENT_NAME,\n",
        "        output_uri_prefix=BUCKET_URI + \"/response-eval\",\n",
        "    )\n",
        "    \n",
        "    print(\"Running response quality evaluation...\")\n",
        "    response_result = response_eval_task.evaluate(\n",
        "        runnable=run_fleet_safety_agent_sync,\n",
        "        experiment_run_name=EXPERIMENT_RUN\n",
        "    )\n",
        "    \n",
        "    print(\"\\n### Summary Metrics\")\n",
        "    display(pd.DataFrame(response_result.summary_metrics.items(), columns=[\"Metric\", \"Value\"]))\n",
        "else:\n",
        "    print(\"Vertex AI evaluation disabled. Set ENABLE_VERTEX_EVAL = True to run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Summary\n",
        "\n",
        "This notebook provides:\n",
        "\n",
        "1. **Local Testing** (Sections 2-5): Direct agent invocation without cloud dependencies\n",
        "2. **Vertex AI Evaluation** (Section 6): Production-grade evaluation with trajectory and response metrics\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Add more test cases to `eval_data` for comprehensive coverage\n",
        "- Create custom metrics for fleet-safety-specific evaluation criteria\n",
        "- Integrate with CI/CD pipeline using `make eval`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
